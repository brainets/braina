{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Information Theory Foundations\n",
    "\n",
    "## Building the Mathematical Groundwork for Understanding Multivariate Information\n",
    "\n",
    "Welcome to the first notebook in our series on multivariate information theory! In this notebook, we'll build the fundamental concepts from the ground up. Think of this as laying the foundation of a house‚Äîwe need solid basics before we can construct more complex structures.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "1. What entropy really measures (and why it's not just \"randomness\")\n",
    "2. How to compute entropy for discrete and continuous variables\n",
    "3. Joint entropy and conditional entropy\n",
    "4. Mutual information as a measure of dependence\n",
    "5. Conditional mutual information\n",
    "\n",
    "### Why Start From Scratch?\n",
    "\n",
    "While we'll eventually use powerful packages like HOI, Frites, and XGI, implementing these concepts ourselves first provides crucial intuition. When the more advanced measures give surprising results later (like negative interaction information!), you'll understand why.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our foundational libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.special import digamma\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make plots beautiful\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Entropy\n",
    "\n",
    "### The Intuition Behind Entropy\n",
    "\n",
    "Entropy measures **uncertainty** or **surprise**. Think about these scenarios:\n",
    "\n",
    "**Scenario A**: You flip a fair coin. Before observing, you're maximally uncertain about the outcome.\n",
    "\n",
    "**Scenario B**: You flip a heavily biased coin that lands heads 99% of the time. You're pretty sure it'll be heads.\n",
    "\n",
    "Scenario A has **higher entropy** because the outcome is more uncertain. This is the core idea: entropy quantifies how much we don't know.\n",
    "\n",
    "### The Mathematical Definition\n",
    "\n",
    "For a discrete random variable $X$ with possible values $x_1, x_2, ..., x_n$:\n",
    "\n",
    "$$H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i)$$\n",
    "\n",
    "The $\\log_2$ means we measure entropy in **bits**. One bit is the information gained from one fair coin flip.\n",
    "\n",
    "### Why the Negative Sign?\n",
    "\n",
    "Probabilities are between 0 and 1, so $\\log_2(P(x_i))$ is negative. The negative sign makes entropy positive, which matches our intuition that \"uncertainty\" should be a positive quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probabilities):\n",
    "    \"\"\"\n",
    "    Calculate entropy for a discrete probability distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    probabilities : array-like\n",
    "        Probability distribution (must sum to 1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    H : float\n",
    "        Entropy in bits\n",
    "        \n",
    "    Notes:\n",
    "    ------\n",
    "    We handle the case where P(x) = 0 by defining 0 * log(0) = 0,\n",
    "    which is the correct limiting value.\n",
    "    \"\"\"\n",
    "    probs = np.array(probabilities)\n",
    "    \n",
    "    # Verify this is a valid probability distribution\n",
    "    assert np.abs(probs.sum() - 1.0) < 1e-10, \"Probabilities must sum to 1\"\n",
    "    assert np.all(probs >= 0), \"Probabilities must be non-negative\"\n",
    "    \n",
    "    # Remove zero probabilities to avoid log(0)\n",
    "    probs = probs[probs > 0]\n",
    "    \n",
    "    # Calculate entropy\n",
    "    H = -np.sum(probs * np.log2(probs))\n",
    "    \n",
    "    return H\n",
    "\n",
    "# Let's test our function with the coin examples\n",
    "fair_coin = [0.5, 0.5]  # 50-50 chance\n",
    "biased_coin = [0.99, 0.01]  # 99% heads\n",
    "certain = [1.0, 0.0]  # Always heads\n",
    "\n",
    "print(\"Entropy Examples:\")\n",
    "print(f\"Fair coin (50-50): {entropy(fair_coin):.4f} bits\")\n",
    "print(f\"Biased coin (99-1): {entropy(biased_coin):.4f} bits\")\n",
    "print(f\"Certain outcome: {entropy(certain):.4f} bits\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Fair coin: Maximum entropy (1 bit) - maximum uncertainty\")\n",
    "print(\"- Biased coin: Low entropy - we're pretty sure what will happen\")\n",
    "print(\"- Certain: Zero entropy - no uncertainty at all!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Entropy as a Function of Probability\n",
    "\n",
    "Let's see how entropy changes as we vary the probability of a binary event. This classic curve shows that entropy is **maximized** when outcomes are equally likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of probabilities for heads\n",
    "p_heads = np.linspace(0.01, 0.99, 100)\n",
    "entropies = [entropy([p, 1-p]) for p in p_heads]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(p_heads, entropies, linewidth=3, color='#2E86AB')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, \n",
    "           label='Maximum entropy (1 bit)')\n",
    "ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "ax.scatter([0.5], [1.0], color='red', s=200, zorder=5, \n",
    "           label='Fair coin', marker='*')\n",
    "\n",
    "# Annotations\n",
    "ax.annotate('Maximum uncertainty\\nat P=0.5', xy=(0.5, 1.0), \n",
    "            xytext=(0.65, 0.85),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('P(Heads)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Entropy (bits)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Binary Entropy Function: H(p)', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight:\")\n",
    "print(\"Entropy is maximized when we're most uncertain (P=0.5 for binary events)\")\n",
    "print(\"As we become more certain (P‚Üí0 or P‚Üí1), entropy decreases to zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Joint and Conditional Entropy\n",
    "\n",
    "### Joint Entropy: Uncertainty About Multiple Variables\n",
    "\n",
    "When we have two random variables $X$ and $Y$, their **joint entropy** $H(X, Y)$ measures the total uncertainty about both variables together:\n",
    "\n",
    "$$H(X,Y) = -\\sum_{i,j} P(x_i, y_j) \\log_2 P(x_i, y_j)$$\n",
    "\n",
    "### Conditional Entropy: Uncertainty After Learning Something\n",
    "\n",
    "**Conditional entropy** $H(X|Y)$ measures how much uncertainty remains about $X$ after we learn $Y$:\n",
    "\n",
    "$$H(X|Y) = -\\sum_{i,j} P(x_i, y_j) \\log_2 P(x_i | y_j)$$\n",
    "\n",
    "Or equivalently:\n",
    "$$H(X|Y) = H(X,Y) - H(Y)$$\n",
    "\n",
    "This is the **chain rule** of entropy!\n",
    "\n",
    "### A Concrete Example: The Beer Price Problem\n",
    "\n",
    "Let's implement the example from your slides: predicting beer prices based on volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_entropy(joint_probs):\n",
    "    \"\"\"\n",
    "    Calculate joint entropy H(X,Y) from a joint probability distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    joint_probs : 2D array\n",
    "        Joint probability matrix P(X=i, Y=j)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    H : float\n",
    "        Joint entropy in bits\n",
    "    \"\"\"\n",
    "    jp = np.array(joint_probs)\n",
    "    \n",
    "    # Verify it's a valid joint distribution\n",
    "    assert np.abs(jp.sum() - 1.0) < 1e-10, \"Joint probabilities must sum to 1\"\n",
    "    \n",
    "    # Remove zeros and calculate\n",
    "    jp_nonzero = jp[jp > 0]\n",
    "    return -np.sum(jp_nonzero * np.log2(jp_nonzero))\n",
    "\n",
    "\n",
    "def conditional_entropy(joint_probs):\n",
    "    \"\"\"\n",
    "    Calculate conditional entropy H(X|Y) from joint distribution.\n",
    "    \n",
    "    We use: H(X|Y) = H(X,Y) - H(Y)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    joint_probs : 2D array\n",
    "        Joint probability matrix P(X=i, Y=j)\n",
    "        Rows correspond to X, columns to Y\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    H_X_given_Y : float\n",
    "        Conditional entropy H(X|Y) in bits\n",
    "    \"\"\"\n",
    "    jp = np.array(joint_probs)\n",
    "    \n",
    "    # Calculate H(X,Y)\n",
    "    H_XY = joint_entropy(jp)\n",
    "    \n",
    "    # Calculate H(Y) from marginal\n",
    "    marginal_Y = jp.sum(axis=0)  # Sum over rows to get P(Y)\n",
    "    H_Y = entropy(marginal_Y)\n",
    "    \n",
    "    # Apply chain rule\n",
    "    return H_XY - H_Y\n",
    "\n",
    "\n",
    "# Example: Beer prices (X) and volumes (Y)\n",
    "# Let's create a realistic joint distribution\n",
    "# Rows = price (low, medium, high), Columns = volume (small, large)\n",
    "\n",
    "beer_joint = np.array([\n",
    "    # Small (0.25L)  Large (0.5L)\n",
    "    [0.20,          0.05],  # Low price (3 euros)\n",
    "    [0.35,          0.15],  # Medium price (5 euros)  \n",
    "    [0.05,          0.20],  # High price (7 euros)\n",
    "])\n",
    "\n",
    "print(\"Beer Price Example\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nJoint Distribution P(Price, Volume):\")\n",
    "print(\"                Small(0.25L)  Large(0.5L)\")\n",
    "print(f\"Low (3‚Ç¨):       {beer_joint[0,0]:.2f}         {beer_joint[0,1]:.2f}\")\n",
    "print(f\"Medium (5‚Ç¨):    {beer_joint[1,0]:.2f}         {beer_joint[1,1]:.2f}\")\n",
    "print(f\"High (7‚Ç¨):      {beer_joint[2,0]:.2f}         {beer_joint[2,1]:.2f}\")\n",
    "\n",
    "# Calculate entropies\n",
    "marginal_price = beer_joint.sum(axis=1)  # P(Price)\n",
    "marginal_volume = beer_joint.sum(axis=0)  # P(Volume)\n",
    "\n",
    "H_price = entropy(marginal_price)\n",
    "H_volume = entropy(marginal_volume)\n",
    "H_joint = joint_entropy(beer_joint)\n",
    "H_price_given_volume = conditional_entropy(beer_joint)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Entropy Calculations:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"H(Price) = {H_price:.4f} bits\")\n",
    "print(f\"H(Volume) = {H_volume:.4f} bits\")\n",
    "print(f\"H(Price, Volume) = {H_joint:.4f} bits\")\n",
    "print(f\"H(Price|Volume) = {H_price_given_volume:.4f} bits\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"- Before knowing volume: {H_price:.4f} bits of uncertainty about price\")\n",
    "print(f\"- After knowing volume: {H_price_given_volume:.4f} bits remain\")\n",
    "print(f\"- Uncertainty reduced by {H_price - H_price_given_volume:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Mutual Information\n",
    "\n",
    "### The Key Measure of Dependence\n",
    "\n",
    "**Mutual Information (MI)** quantifies how much knowing one variable reduces uncertainty about another:\n",
    "\n",
    "$$I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$\n",
    "\n",
    "The symmetry $(I(X;Y) = I(Y;X))$ is beautiful: the information $X$ provides about $Y$ equals the information $Y$ provides about $X$.\n",
    "\n",
    "### Alternative Formulation: KL Divergence\n",
    "\n",
    "MI can also be written as:\n",
    "\n",
    "$$I(X;Y) = \\sum_{i,j} P(x_i, y_j) \\log_2 \\frac{P(x_i, y_j)}{P(x_i)P(y_j)}$$\n",
    "\n",
    "This shows MI measures how far the joint distribution is from independence. If $X$ and $Y$ are independent: $P(X,Y) = P(X)P(Y)$, so $I(X;Y) = 0$.\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Non-negative**: $I(X;Y) \\geq 0$ always\n",
    "2. **Zero iff independent**: $I(X;Y) = 0 \\iff X \\perp Y$\n",
    "3. **Bounded**: $I(X;Y) \\leq \\min(H(X), H(Y))$\n",
    "4. **Symmetric**: $I(X;Y) = I(Y;X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(joint_probs):\n",
    "    \"\"\"\n",
    "    Calculate mutual information I(X;Y) from joint distribution.\n",
    "    \n",
    "    Uses: I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    joint_probs : 2D array\n",
    "        Joint probability matrix P(X=i, Y=j)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    MI : float\n",
    "        Mutual information in bits\n",
    "    \"\"\"\n",
    "    jp = np.array(joint_probs)\n",
    "    \n",
    "    # Calculate marginals\n",
    "    marginal_X = jp.sum(axis=1)\n",
    "    marginal_Y = jp.sum(axis=0)\n",
    "    \n",
    "    # Calculate individual and joint entropies\n",
    "    H_X = entropy(marginal_X)\n",
    "    H_Y = entropy(marginal_Y)\n",
    "    H_XY = joint_entropy(jp)\n",
    "    \n",
    "    # MI = H(X) + H(Y) - H(X,Y)\n",
    "    return H_X + H_Y - H_XY\n",
    "\n",
    "\n",
    "# Calculate MI for our beer example\n",
    "MI_beer = mutual_information(beer_joint)\n",
    "\n",
    "print(\"Mutual Information Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nI(Price; Volume) = {MI_beer:.4f} bits\")\n",
    "print(\"\\nVerification (should match):\")\n",
    "print(f\"H(Price) - H(Price|Volume) = {H_price - H_price_given_volume:.4f} bits ‚úì\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"Knowing the volume reduces price uncertainty by {MI_beer:.4f} bits\")\n",
    "print(f\"Equivalently, knowing price reduces volume uncertainty by {MI_beer:.4f} bits\")\n",
    "\n",
    "# Let's also create examples of different dependency strengths\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Comparing Different Dependency Strengths\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Perfect independence\n",
    "independent = np.outer([0.25, 0.75], [0.4, 0.6])\n",
    "MI_indep = mutual_information(independent)\n",
    "\n",
    "# Perfect dependence (deterministic)\n",
    "deterministic = np.array([\n",
    "    [0.5, 0.0],\n",
    "    [0.0, 0.5]\n",
    "])\n",
    "MI_det = mutual_information(deterministic)\n",
    "\n",
    "print(f\"\\n1. Independent variables: I(X;Y) = {MI_indep:.6f} bits ‚âà 0\")\n",
    "print(f\"2. Our beer example: I(X;Y) = {MI_beer:.4f} bits (moderate dependence)\")\n",
    "print(f\"3. Deterministic relation: I(X;Y) = {MI_det:.4f} bit (perfect dependence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Venn Diagram of Information\n",
    "\n",
    "The classic \"information diagram\" helps visualize the relationship between entropy and mutual information. This works perfectly for two variables (but we'll see later why it breaks for three!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle, Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "def plot_information_venn(H_X, H_Y, MI):\n",
    "    \"\"\"\n",
    "    Create a Venn diagram representation of information measures.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Calculate regions\n",
    "    H_X_given_Y = H_X - MI\n",
    "    H_Y_given_X = H_Y - MI\n",
    "    \n",
    "    # Draw circles\n",
    "    circle1 = Circle((0.3, 0.5), 0.25, alpha=0.5, color='#E63946', label=f'H(X)={H_X:.3f}')\n",
    "    circle2 = Circle((0.7, 0.5), 0.25, alpha=0.5, color='#457B9D', label=f'H(Y)={H_Y:.3f}')\n",
    "    ax.add_patch(circle1)\n",
    "    ax.add_patch(circle2)\n",
    "    \n",
    "    # Add text labels\n",
    "    ax.text(0.15, 0.5, f'H(X|Y)\\n{H_X_given_Y:.3f}', \n",
    "            fontsize=11, ha='center', va='center', fontweight='bold')\n",
    "    ax.text(0.5, 0.5, f'I(X;Y)\\n{MI:.3f}', \n",
    "            fontsize=12, ha='center', va='center', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "    ax.text(0.85, 0.5, f'H(Y|X)\\n{H_Y_given_X:.3f}', \n",
    "            fontsize=11, ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0.1, 0.9)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Information Venn Diagram', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=2, fontsize=12)\n",
    "    \n",
    "    # Add equations at bottom\n",
    "    eq_text = (\n",
    "        f\"H(X,Y) = H(X|Y) + I(X;Y) + H(Y|X)\\n\"\n",
    "        f\"       = {H_X_given_Y:.3f} + {MI:.3f} + {H_Y_given_X:.3f} = {H_X_given_Y + MI + H_Y_given_X:.3f} bits\"\n",
    "    )\n",
    "    fig.text(0.5, 0.05, eq_text, ha='center', fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# Plot for our beer example\n",
    "fig, ax = plot_information_venn(H_price, H_volume, MI_beer)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"The overlapping region represents shared information (mutual information).\")\n",
    "print(\"The non-overlapping regions are information unique to each variable.\")\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: This diagram works for 2 variables, but breaks for 3+!\")\n",
    "print(\"    We'll see why in the next notebook when we encounter negative information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Conditional Mutual Information\n",
    "\n",
    "### Beyond Pairwise Dependencies\n",
    "\n",
    "**Conditional Mutual Information (CMI)** measures the information shared between $X$ and $Y$ after accounting for a third variable $Z$:\n",
    "\n",
    "$$I(X;Y|Z) = H(X|Z) - H(X|Y,Z)$$\n",
    "\n",
    "Equivalently:\n",
    "$$I(X;Y|Z) = H(X|Z) + H(Y|Z) - H(X,Y|Z)$$\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "CMI is crucial for understanding **multivariate** dependencies. It answers: \"After I know $Z$, how much does learning $Y$ still help me predict $X$?\"\n",
    "\n",
    "### A Surprising Property: CMI Can Increase!\n",
    "\n",
    "Unlike correlation (which conditioning always reduces), CMI can actually **increase** when we condition. This is called **explaining away** and we'll explore it in the next notebook.\n",
    "\n",
    "For now, let's implement CMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_mutual_information(joint_probs_XYZ):\n",
    "    \"\"\"\n",
    "    Calculate conditional mutual information I(X;Y|Z).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    joint_probs_XYZ : 3D array\n",
    "        Joint probability P(X, Y, Z)\n",
    "        Shape: (n_X, n_Y, n_Z)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    CMI : float\n",
    "        Conditional mutual information I(X;Y|Z) in bits\n",
    "        \n",
    "    Formula:\n",
    "    --------\n",
    "    I(X;Y|Z) = sum_z P(z) * I(X;Y|Z=z)\n",
    "             = H(X|Z) + H(Y|Z) - H(X,Y|Z)\n",
    "    \"\"\"\n",
    "    jp_XYZ = np.array(joint_probs_XYZ)\n",
    "    n_X, n_Y, n_Z = jp_XYZ.shape\n",
    "    \n",
    "    # Calculate marginal P(Z)\n",
    "    P_Z = jp_XYZ.sum(axis=(0, 1))  # Sum over X and Y\n",
    "    \n",
    "    # Initialize CMI\n",
    "    CMI = 0.0\n",
    "    \n",
    "    # For each value of Z, calculate I(X;Y|Z=z) and weight by P(z)\n",
    "    for z in range(n_Z):\n",
    "        if P_Z[z] > 0:\n",
    "            # Conditional joint distribution P(X,Y|Z=z)\n",
    "            P_XY_given_z = jp_XYZ[:, :, z] / P_Z[z]\n",
    "            \n",
    "            # Calculate MI for this conditioning value\n",
    "            MI_given_z = mutual_information(P_XY_given_z)\n",
    "            \n",
    "            # Weight by P(z)\n",
    "            CMI += P_Z[z] * MI_given_z\n",
    "    \n",
    "    return CMI\n",
    "\n",
    "\n",
    "# Example: Coffee shop scenario\n",
    "# X = Weather (Rainy, Sunny)\n",
    "# Y = Coffee sales (Low, High) \n",
    "# Z = Day type (Weekday, Weekend)\n",
    "\n",
    "# Create a realistic 3D joint distribution\n",
    "coffee_joint = np.zeros((2, 2, 2))  # (weather, sales, day_type)\n",
    "\n",
    "# Weekday (Z=0): Weather affects sales more\n",
    "coffee_joint[:, :, 0] = np.array([\n",
    "    # Low sales  High sales\n",
    "    [0.15,       0.05],  # Rainy\n",
    "    [0.05,       0.15],  # Sunny\n",
    "])\n",
    "\n",
    "# Weekend (Z=1): Sales always high regardless of weather\n",
    "coffee_joint[:, :, 1] = np.array([\n",
    "    # Low sales  High sales\n",
    "    [0.05,       0.25],  # Rainy\n",
    "    [0.05,       0.25],  # Sunny\n",
    "])\n",
    "\n",
    "# Calculate various MIs\n",
    "marginal_XY = coffee_joint.sum(axis=2)  # P(Weather, Sales)\n",
    "MI_weather_sales = mutual_information(marginal_XY)\n",
    "CMI_weather_sales_given_day = conditional_mutual_information(coffee_joint)\n",
    "\n",
    "print(\"Coffee Shop Example: Weather ‚Üí Sales\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nI(Weather; Sales) = {MI_weather_sales:.4f} bits\")\n",
    "print(f\"I(Weather; Sales | DayType) = {CMI_weather_sales_given_day:.4f} bits\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Overall: Weather and sales share some information\")\n",
    "print(\"- After knowing if it's a weekday/weekend:\")\n",
    "if CMI_weather_sales_given_day > MI_weather_sales:\n",
    "    print(\"  The relationship is STRONGER (explaining away effect!)\")\n",
    "elif CMI_weather_sales_given_day < MI_weather_sales:\n",
    "    print(\"  The relationship is WEAKER (day type explains some dependence)\")\n",
    "else:\n",
    "    print(\"  The relationship is UNCHANGED (day type is irrelevant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Estimating Information from Data\n",
    "\n",
    "### From Theory to Practice\n",
    "\n",
    "So far we've worked with **known** probability distributions. In real research, we have **data samples** and need to **estimate** these distributions.\n",
    "\n",
    "### Three Common Approaches\n",
    "\n",
    "1. **Histogram/Binning**: Discretize continuous data into bins\n",
    "2. **Kernel Density Estimation (KDE)**: Smooth density estimation\n",
    "3. **k-Nearest Neighbors (KNN)**: Distance-based estimation\n",
    "\n",
    "Each has trade-offs in bias, variance, and computational cost. Let's implement the simplest approach (binning) and understand its limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_entropy_binned(data, n_bins=10):\n",
    "    \"\"\"\n",
    "    Estimate entropy from continuous data using binning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        1D array of samples\n",
    "    n_bins : int\n",
    "        Number of bins for histogram\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    H : float\n",
    "        Estimated entropy in bits\n",
    "        \n",
    "    Warning:\n",
    "    --------\n",
    "    This is a biased estimator! The choice of n_bins matters.\n",
    "    \"\"\"\n",
    "    # Create histogram\n",
    "    counts, _ = np.histogram(data, bins=n_bins)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = counts / counts.sum()\n",
    "    \n",
    "    # Remove zero bins\n",
    "    probs = probs[probs > 0]\n",
    "    \n",
    "    return entropy(probs)\n",
    "\n",
    "\n",
    "def estimate_mi_binned(X, Y, n_bins=10):\n",
    "    \"\"\"\n",
    "    Estimate mutual information from samples using 2D histogram.\n",
    "    \"\"\"\n",
    "    # Create 2D histogram for joint distribution\n",
    "    joint_counts, _, _ = np.histogram2d(X, Y, bins=n_bins)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    joint_probs = joint_counts / joint_counts.sum()\n",
    "    \n",
    "    return mutual_information(joint_probs)\n",
    "\n",
    "\n",
    "# Generate some sample data with known MI\n",
    "n_samples = 5000\n",
    "\n",
    "# Create correlated Gaussian variables\n",
    "rho = 0.7  # Correlation coefficient\n",
    "mean = [0, 0]\n",
    "cov = [[1, rho], [rho, 1]]\n",
    "data = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "X_data = data[:, 0]\n",
    "Y_data = data[:, 1]\n",
    "\n",
    "# For Gaussians, MI has an exact formula: I(X;Y) = -0.5 * log(1 - rho¬≤)\n",
    "true_MI = -0.5 * np.log2(1 - rho**2)\n",
    "\n",
    "print(\"Estimating MI from Data\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTrue MI (Gaussian formula): {true_MI:.4f} bits\")\n",
    "print(\"\\nEstimated MI with different bin sizes:\")\n",
    "\n",
    "for n_bins in [5, 10, 20, 30, 50]:\n",
    "    est_MI = estimate_mi_binned(X_data, Y_data, n_bins)\n",
    "    error = abs(est_MI - true_MI)\n",
    "    print(f\"  {n_bins:2d} bins: {est_MI:.4f} bits (error: {error:.4f})\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Key Lesson:\")\n",
    "print(\"Bin size matters! Too few bins ‚Üí underestimate, too many ‚Üí noise\")\n",
    "print(\"This is why advanced packages (like Frites and HOI) use better methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Bias-Variance Trade-off\n",
    "\n",
    "Let's see how our MI estimate varies with bin size and sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different sample sizes and bin numbers\n",
    "sample_sizes = [100, 500, 1000, 5000, 10000]\n",
    "bin_numbers = np.arange(5, 51, 5)\n",
    "\n",
    "# Store results\n",
    "results = np.zeros((len(sample_sizes), len(bin_numbers)))\n",
    "\n",
    "for i, n_samples in enumerate(sample_sizes):\n",
    "    # Generate data\n",
    "    data = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "    X = data[:, 0]\n",
    "    Y = data[:, 1]\n",
    "    \n",
    "    for j, n_bins in enumerate(bin_numbers):\n",
    "        results[i, j] = estimate_mi_binned(X, Y, n_bins)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "for i, n_samples in enumerate(sample_sizes):\n",
    "    ax.plot(bin_numbers, results[i, :], marker='o', linewidth=2,\n",
    "            label=f'N = {n_samples}', alpha=0.7)\n",
    "\n",
    "ax.axhline(y=true_MI, color='black', linestyle='--', linewidth=2.5,\n",
    "           label=f'True MI = {true_MI:.4f} bits')\n",
    "\n",
    "ax.set_xlabel('Number of Bins', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Estimated MI (bits)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('MI Estimation: Effect of Sample Size and Binning', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"1. Small sample sizes ‚Üí high variance (estimates jump around)\")\n",
    "print(\"2. Too few bins ‚Üí systematic underestimation (bias)\")\n",
    "print(\"3. Too many bins with small samples ‚Üí overestimation (sparse bins)\")\n",
    "print(\"4. More data allows more bins and more accurate estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Summary and Bridge to Advanced Methods\n",
    "\n",
    "### What We've Built\n",
    "\n",
    "Congratulations! You now understand:\n",
    "\n",
    "1. **Entropy**: Measures uncertainty/surprise\n",
    "2. **Joint and Conditional Entropy**: Uncertainty about multiple variables\n",
    "3. **Mutual Information**: How much variables tell us about each other\n",
    "4. **Conditional MI**: Information shared after accounting for a third variable\n",
    "5. **Estimation Challenges**: Why we need sophisticated methods\n",
    "\n",
    "### The Foundations Are Set\n",
    "\n",
    "These concepts form the bedrock of multivariate information theory. In the next notebooks, we'll:\n",
    "\n",
    "- **Notebook 2**: See why pairwise measures fail (the XOR problem)\n",
    "- **Notebook 3**: Learn about synergy, redundancy, and interaction information using **HOI**\n",
    "- **Notebook 4**: Apply these to real neural time series with **Frites**\n",
    "- **Notebook 5**: Explore higher-order network structures with **XGI**\n",
    "- **Notebook 6**: Integrate everything for advanced analyses\n",
    "\n",
    "### Practice Exercises\n",
    "\n",
    "Before moving on, try these to solidify your understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PRACTICE EXERCISES\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n1. Calculate the entropy of a fair 6-sided die.\")\n",
    "print(\"   Hint: Each outcome has probability 1/6\")\n",
    "fair_die = np.ones(6) / 6\n",
    "H_die = entropy(fair_die)\n",
    "print(f\"   Answer: {H_die:.4f} bits\")\n",
    "print(f\"   Compare to fair coin (1 bit): You need {H_die:.2f}/1.0 = {H_die:.2f}x as many dice rolls\")\n",
    "print(\"   to match the information from a coin flip!\\n\")\n",
    "\n",
    "print(\"2. Why is H(X,Y) ‚â§ H(X) + H(Y)?\")\n",
    "print(\"   This is called subadditivity. Can you explain why?\")\n",
    "print(\"   Hint: When are they equal? When are they most different?\\n\")\n",
    "\n",
    "print(\"3. Create two variables X and Y where I(X;Y) = H(X).\")\n",
    "print(\"   What does this mean about the relationship between X and Y?\")\n",
    "# Example: Perfect dependence\n",
    "perfect_dep = np.array([[0.3, 0.0], [0.0, 0.7]])\n",
    "MI_perfect = mutual_information(perfect_dep)\n",
    "H_X_perfect = entropy(perfect_dep.sum(axis=1))\n",
    "print(f\"   Example MI: {MI_perfect:.4f}, H(X): {H_X_perfect:.4f}\")\n",
    "print(f\"   This means: Y completely determines X (or vice versa)!\\n\")\n",
    "\n",
    "print(\"4. Generate two independent random variables and verify I(X;Y) ‚âà 0.\")\n",
    "X_indep = np.random.randn(1000)\n",
    "Y_indep = np.random.randn(1000)\n",
    "MI_indep_est = estimate_mi_binned(X_indep, Y_indep, n_bins=20)\n",
    "print(f\"   Estimated MI: {MI_indep_est:.6f} bits (should be near 0)\")\n",
    "print(\"   Small non-zero value is due to finite sample size!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Ready for Notebook 2: The XOR Problem!\")\n",
    "print(\"There we'll see why everything breaks with 3+ variables...\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Recommended Reading\n",
    "\n",
    "1. **Cover & Thomas** - \"Elements of Information Theory\" (Chapters 2-3)\n",
    "2. **MacKay** - \"Information Theory, Inference, and Learning Algorithms\" (Chapter 2)\n",
    "3. **Ince et al. 2017** - \"A statistical framework for neuroimaging data analysis based on mutual information\"\n",
    "\n",
    "### Going Deeper\n",
    "\n",
    "- For continuous variables: differential entropy\n",
    "- For high-dimensional data: dimensionality reduction before MI estimation  \n",
    "- For time series: transfer entropy and Granger causality\n",
    "- For neural data: Gaussian Copula MI (what Frites uses!)\n",
    "\n",
    "### Coming Up\n",
    "\n",
    "In the next notebook, we'll confront the **XOR problem**‚Äîthe moment when your intuition about information will be completely challenged. You'll see that two variables can individually carry **zero** information about a target, yet together carry **complete** information. This is the gateway to understanding synergy, redundancy, and higher-order interactions.\n",
    "\n",
    "See you there! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
